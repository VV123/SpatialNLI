{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNP2W7yfkAoB"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install simpletransformers\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1iXthLuGbop",
    "outputId": "705c1aca-52e0-45fb-bab8-1a6ad020a4c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5uczpx-kcCN"
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc\n",
    "from scipy.special import softmax\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold \n",
    "import sklearn\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "#choose the same seed to assure that our model will be roproducible\n",
    "\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_all(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4H4bwDXNkgo0",
    "outputId": "cb4c0e05-c32b-44de-b2ed-03dc575ecab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/sql-jing1\n"
     ]
    }
   ],
   "source": [
    "cd '/content/drive/My Drive/sql-jing1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "d05500e0130046dbbe6feec01cc90e82",
      "46e68125f05e46e6a5858862b792f7fe",
      "7a1519be01e243489a4fc653286b2c2f",
      "4603da02e25442eab88908d4bdf3e393",
      "027ffb7c707e4464b5d0466cdb9d5f28",
      "033ebe4d04c74c84b263ff05358f74a7",
      "9efd6fe1c5014322a3a95bdad7318d75",
      "e68fde583a374186b942be767bd45248"
     ]
    },
    "id": "lV4rhGJmktYu",
    "outputId": "b862c628-f574-4743-cd34-f3048067ddd5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05500e0130046dbbe6feec01cc90e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1647\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import BertTokenizer, BertModel\n",
    "Y = ['mountain', 'border', 'high', 'city', 'state', 'river']\n",
    "def read_file(data='train',batch_size=16,max_len=22):\n",
    "\n",
    "  filename = data + '_org.qu'\n",
    "  x, xt, y = [], [], []\n",
    "  sent_x, sent_t = [], []\n",
    "\n",
    "  with open(filename) as f:\n",
    "    ls = f.readlines()\n",
    "  total = len(ls)\n",
    "  num_batch = int((total+batch_size)/batch_size)\n",
    "  for i in range(num_batch):\n",
    "    st = batch_size * i\n",
    "    ed = min(batch_size * i + batch_size, total)\n",
    "    x_batch, xt_batch, y_batch = [], [], []\n",
    "    sent_x_batch, sent_t_batch = [], []\n",
    "    for j in range(st, ed):\n",
    "      l = ls[j]\n",
    "      tables = l.strip('\\n').split('\\t')[0].split()\n",
    "      tables = [t.replace('table_','').replace('_info','') for t in tables]\n",
    "      \n",
    "      question = l.strip('\\n').split('\\t')[1]\n",
    "      tokenized_input = tokenizer.tokenize(question) \n",
    "      while len(tokenized_input) < max_len:\n",
    "          tokenized_input.append('[PAD]')\n",
    "      x_idx = tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "      #positive samples\n",
    "      for t in tables:\n",
    "        sent_x_batch.append(question)\n",
    "        #print(x_idx)\n",
    "        x_batch.append(x_idx)\n",
    "\n",
    "        sent_t_batch.append(t)\n",
    "        xt_idx = tokenizer.convert_tokens_to_ids([t])\n",
    "        xt_batch.append(xt_idx)\n",
    "        y_batch.append(1.0)\n",
    "\n",
    "      #negative sampling\n",
    "      YY = [t for t in Y if t not in tables]\n",
    "      for t in YY:\n",
    "        sent_x_batch.append(question)\n",
    "        sent_t_batch.append(t)\n",
    "        xt_idx = tokenizer.convert_tokens_to_ids([t])\n",
    "        x_batch.append(x_idx)\n",
    "        xt_batch.append(xt_idx)\n",
    "        y_batch.append(0.0)\n",
    "\n",
    "      #positive sampling for balance\n",
    "      if data == '.':\n",
    "        for j in range(len(YY)-len(tables)):\n",
    "          t = random.choice(tables)\n",
    "          sent_x_batch.append(question)\n",
    "          sent_t_batch.append(t)\n",
    "          x_batch.append(x_idx)\n",
    "          xt_idx = tokenizer.convert_tokens_to_ids([t])\n",
    "          xt_batch.append(xt_idx)\n",
    "          y_batch.append(1.0)\n",
    "\n",
    "    x_batch = np.asarray(x_batch)\n",
    "    #print(x_batch.shape)\n",
    "    x_batch = np.transpose(x_batch, (1,0))\n",
    "    xt_batch = np.asarray(xt_batch)\n",
    "    xt_batch = np.transpose(xt_batch, (1,0))\n",
    "    y_batch = np.asarray(y_batch)\n",
    "    sent_x_batch = np.asarray(sent_x_batch)\n",
    "    sent_t_batch = np.asarray(sent_t_batch)\n",
    "\n",
    "    x.append(torch.tensor(x_batch, dtype=torch.long, device=device))\n",
    "    xt.append(torch.tensor(xt_batch, dtype=torch.long, device=device))\n",
    "    y.append(torch.tensor(y_batch, dtype=torch.float, device=device))\n",
    "    sent_x.append(sent_x_batch)\n",
    "    sent_t.append(sent_t_batch)\n",
    "  print(total)\n",
    "  return x, xt, y, sent_x, sent_t\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_x, train_xt, train_y, _, _ = read_file('train')\n",
    "assert len(train_x) == len(train_xt) == len(train_y)\n",
    "test_x, test_xt, test_y, sent_x, sent_t = read_file('test')\n",
    "assert len(test_x) == len(test_xt) == len(test_y) == len(sent_x) == len(sent_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4zMZ6mecF0M",
    "outputId": "2e0020b3-15e9-42b4-a3ce-c9d72e79adce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train positive 1890.0 total 9897\n",
      "test positive 329.0 total 1674\n"
     ]
    }
   ],
   "source": [
    "pos, ttl = 0, 0\n",
    "for l in train_y:\n",
    "  ttl += l.size(0)\n",
    "  pos += sum(l)\n",
    "print('train positive {0} total {1}'.format(pos, ttl))\n",
    "pos, ttl = 0, 0\n",
    "for l in test_y:\n",
    "  ttl += l.size(0)\n",
    "  pos += sum(l)\n",
    "print('test positive {0} total {1}'.format(pos, ttl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_pYnU6jsgBy"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        print('New bert embedding')\n",
    "        #bertmodel = BertModel.from_pretrained('bert-base-uncased')\n",
    "        #bertmodel.resize_token_embeddings(input_dim)\n",
    "        #self.embedding = bertmodel.embeddings\n",
    "        self.embedding = emb\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, emb, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = emb\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, 1)\n",
    "        #self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        #input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "\n",
    "        #print(embedded.size())\n",
    "        #print(weighted.size())\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "  \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        #prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7V4n2PquDGa"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "          super().__init__()\n",
    "        \n",
    "          self.encoder = encoder\n",
    "          self.decoder = decoder\n",
    "          self.device = device\n",
    "          self.output_layer = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg\n",
    "        #input =  torch.tensor([101]*BATCH_SIZE, device=device)\n",
    "          \n",
    "        #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "        #receive output tensor (predictions) and new hidden state\n",
    "        output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "        output = output.squeeze(1)\n",
    "        output = self.output_layer(output)\n",
    "          \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149,
     "referenced_widgets": [
      "0e9ccd7df1c346f18c08d693c020201a",
      "1dd4bd60c2a14a48a8874fd6e1ff0198",
      "afa77d82ab9c4470ac6cb91ff450ae13",
      "19f87b9c354e48bfa9c1aa1821515401",
      "39701143bf1e436f92ccd57b8efac354",
      "d707fd4a3b414c30a629b6cdc0285765",
      "9a8c58ebba4947ccb7ab53920bcde607",
      "1359b51272df477995c20c8724810861",
      "0254095923c647d8986487d78b625d1f",
      "444dbba966d14c95977dc9980181dddb",
      "f4a603b606514cbcaa4af43f08e02682",
      "e4021fcf00ce49148b663644b96b955a",
      "248a52af67a14c01b1900202fc1815a4",
      "708f47eda91246d9ae51b75959256a8c",
      "7116a63b67714e3dbac50646fd72a4ee",
      "2df467bc3fba41b98baa5a5a2e48ac23"
     ]
    },
    "id": "cf8yLVQQvZl6",
    "outputId": "d62d75dd-676c-47ac-fd21-00dd1874bd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9ccd7df1c346f18c08d693c020201a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0254095923c647d8986487d78b625d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New bert embedding\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(tokenizer)\n",
    "ENC_EMB_DIM = 768 \n",
    "DEC_EMB_DIM = 768 \n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "print(INPUT_DIM)\n",
    "\n",
    "bertmodel = BertModel.from_pretrained('bert-base-uncased')\n",
    "emb = bertmodel.embeddings\n",
    "#emb = nn.Embedding(INPUT_DIM, ENC_EMB_DIM)\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(emb, INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(emb, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVyvYDtsP0_v",
    "outputId": "8299a16d-c641-4740-92f5-6da21a2b230a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (rnn): GRU(768, 256, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (rnn): GRU(1280, 256)\n",
       "    (fc_out): Linear(in_features=1536, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (output_layer): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BaHui5tw5kl"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    " \n",
    "    #model.load_state_dict(torch.load('tut3-model.pt'))\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0  \n",
    "    acc = 0\n",
    "    total = 0\n",
    "    right = 0\n",
    "    num_iter = len(train_x)\n",
    "    for i in range(num_iter):\n",
    "    #for i, batch in enumerate(iterator):\n",
    "        #print('{0}/{1}'.format(i,len(train_input), end='\\r'))\n",
    "        src = train_x[i]\n",
    "        trg = train_xt[i]\n",
    "        #src = torch.transpose(src, 1, 0)\n",
    "        #trg = torch.transpose(trg, 1, 0)\n",
    "        #src.transpose_(0,1)\n",
    "        #trg.transpose_(0,1)\n",
    "        \n",
    "        #src = batch.src\n",
    "        #trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "         \n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        #output_dim = output.shape[-1]\n",
    "        \n",
    "        #output = output[1:].view(-1, output_dim)\n",
    "\n",
    "        #trg = trg[1:].view(-1)\n",
    "        #trg = trg[1:].reshape((MAX_LENGTH-1)*trg[1:].size(1))\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, train_y[i])\n",
    "        #print(output)\n",
    "        #print(train_y[i])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        y_pred = np.array(output.cpu().detach())\n",
    "        y_pred = np.where(y_pred <= 0.5, 0, y_pred)\n",
    "        y_pred = np.where(y_pred > 0.5, 1, y_pred)\n",
    "        ybar = train_y[i].cpu().detach().numpy()\n",
    "        acc += sum(y_pred==ybar)\n",
    "        total += len(ybar)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return epoch_loss / num_iter, acc*1.0 / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wjg2a2D614mV"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, PRINT=False):\n",
    "    \n",
    "    w = open('test_org.qu.new', 'w')\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    total = 0\n",
    "    wr = 0\n",
    "    pre = ''\n",
    "    ts = ''\n",
    "    with torch.no_grad():\n",
    "        num_iter = len(test_x)\n",
    "        for i in range(num_iter):\n",
    "        #for i, batch in enumerate(iterator):\n",
    "            src = test_x[i]\n",
    "            trg = test_xt[i]\n",
    "            #src = torch.transpose(src, 1, 0)\n",
    "            #trg = torch.transpose(trg, 1, 0)\n",
    "            \n",
    "            output = model(src, trg) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            #output_dim = output.shape[-1]\n",
    "            \n",
    "            #output = output[1:].view(-1, output_dim)\n",
    "            #trg = trg[1:].view(-1)\n",
    "            #trg = trg[1:].reshape((MAX_LENGTH-1)*trg[1:].size(1))\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, test_y[i])\n",
    "            y_pred = np.array(output.cpu().detach())\n",
    "            y_pred = np.where(y_pred <= 0.5, 0.0, y_pred)\n",
    "            y_pred = np.where(y_pred > 0.5, 1.0, y_pred)\n",
    "            ybar = test_y[i].cpu().detach().numpy()\n",
    "            acc += sum(y_pred==ybar)\n",
    "            total += len(ybar)\n",
    "            epoch_loss += loss.item()\n",
    "            assert len(sent_x[i]) == len(y_pred) == len(ybar) == len(sent_t[i])\n",
    "            if PRINT:\n",
    "              for j in range(len(sent_x[i])):\n",
    "                #print('---------')\n",
    "                #print(sent_x[i][j])\n",
    "                #print(sent_t[i][j])\n",
    "                #print(y_pred[j])\n",
    "                #print(ybar[j])\n",
    "                if pre == '':\n",
    "                  pre = sent_x[i][j]\n",
    "                if pre != '' and pre != sent_x[i][j]:\n",
    "                  w.write(ts.strip() + '\\t' + pre + '\\n')\n",
    "                  pre = sent_x[i][j]\n",
    "                  ts = ''\n",
    "                t = sent_t[i][j]\n",
    "                t = t.replace('border','border_info')\n",
    "                t = t.replace('high','highlow')\n",
    "                t = 'table_' + t\n",
    "                if y_pred[j]==1.0:\n",
    "                  ts += (t + ' ') \n",
    "        w.write(ts.strip() + '\\t' + pre)    \n",
    "    w.close()  \n",
    "    print(f'\\t Val. Loss: {epoch_loss / num_iter:.3f} Test Acc: {acc*1.0 / total:.3f}')      \n",
    "        \n",
    "    return epoch_loss / num_iter, acc*1.0 / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHtqmddb2H3v"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVWIgEUD2Ju9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "N_EPOCHS = 30\n",
    "CLIP = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()  \n",
    "    train_loss, train_acc = train(model, None, optimizer, criterion, CLIP)\n",
    "    valid_loss, valid_acc = evaluate(model, None, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} Train Acc: {train_acc:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} Test Acc: {valid_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPVkA8hRROmF",
    "outputId": "203b3327-1e6a-4dcc-92da-e0b0fca529b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Val. Loss: 0.021 Test Acc: 0.997\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut3-model-997.pt'))\n",
    "valid_loss, valid_acc = evaluate(model, None, criterion, PRINT=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "table_link.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0254095923c647d8986487d78b625d1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4a603b606514cbcaa4af43f08e02682",
       "IPY_MODEL_e4021fcf00ce49148b663644b96b955a"
      ],
      "layout": "IPY_MODEL_444dbba966d14c95977dc9980181dddb"
     }
    },
    "027ffb7c707e4464b5d0466cdb9d5f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "033ebe4d04c74c84b263ff05358f74a7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e9ccd7df1c346f18c08d693c020201a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_afa77d82ab9c4470ac6cb91ff450ae13",
       "IPY_MODEL_19f87b9c354e48bfa9c1aa1821515401"
      ],
      "layout": "IPY_MODEL_1dd4bd60c2a14a48a8874fd6e1ff0198"
     }
    },
    "1359b51272df477995c20c8724810861": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19f87b9c354e48bfa9c1aa1821515401": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1359b51272df477995c20c8724810861",
      "placeholder": "​",
      "style": "IPY_MODEL_9a8c58ebba4947ccb7ab53920bcde607",
      "value": " 433/433 [00:12&lt;00:00, 33.8B/s]"
     }
    },
    "1dd4bd60c2a14a48a8874fd6e1ff0198": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "248a52af67a14c01b1900202fc1815a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2df467bc3fba41b98baa5a5a2e48ac23": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39701143bf1e436f92ccd57b8efac354": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "444dbba966d14c95977dc9980181dddb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4603da02e25442eab88908d4bdf3e393": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e68fde583a374186b942be767bd45248",
      "placeholder": "​",
      "style": "IPY_MODEL_9efd6fe1c5014322a3a95bdad7318d75",
      "value": " 232k/232k [00:00&lt;00:00, 670kB/s]"
     }
    },
    "46e68125f05e46e6a5858862b792f7fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "708f47eda91246d9ae51b75959256a8c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7116a63b67714e3dbac50646fd72a4ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a1519be01e243489a4fc653286b2c2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_033ebe4d04c74c84b263ff05358f74a7",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_027ffb7c707e4464b5d0466cdb9d5f28",
      "value": 231508
     }
    },
    "9a8c58ebba4947ccb7ab53920bcde607": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9efd6fe1c5014322a3a95bdad7318d75": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afa77d82ab9c4470ac6cb91ff450ae13": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d707fd4a3b414c30a629b6cdc0285765",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39701143bf1e436f92ccd57b8efac354",
      "value": 433
     }
    },
    "d05500e0130046dbbe6feec01cc90e82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7a1519be01e243489a4fc653286b2c2f",
       "IPY_MODEL_4603da02e25442eab88908d4bdf3e393"
      ],
      "layout": "IPY_MODEL_46e68125f05e46e6a5858862b792f7fe"
     }
    },
    "d707fd4a3b414c30a629b6cdc0285765": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4021fcf00ce49148b663644b96b955a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2df467bc3fba41b98baa5a5a2e48ac23",
      "placeholder": "​",
      "style": "IPY_MODEL_7116a63b67714e3dbac50646fd72a4ee",
      "value": " 440M/440M [00:12&lt;00:00, 36.5MB/s]"
     }
    },
    "e68fde583a374186b942be767bd45248": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4a603b606514cbcaa4af43f08e02682": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_708f47eda91246d9ae51b75959256a8c",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_248a52af67a14c01b1900202fc1815a4",
      "value": 440473133
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
